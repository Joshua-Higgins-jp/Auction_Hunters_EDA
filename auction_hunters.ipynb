{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4744249-5624-4dd7-bf89-3b41326c37e6",
   "metadata": {},
   "source": [
    "# **Auction Hunters**\n",
    "## Exploratory Data Analysis\n",
    "#### Project started: March, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33aa4a1-02cf-4d4c-91bb-0f1d2ba07a57",
   "metadata": {},
   "source": [
    "## What is *Auction Hunters*?\n",
    "* Auction hunters is a TV show that aired in the USA from 2010-2015. Hosts Allen and Ton travel and bid for abandoned storage units. Each episode shows where they go, what storage units they bid, how much they win them for, and also shows the most exciting items found in each won unit.\n",
    "* In each episode, they find buyers for the most profitable items in the storage units they bid on and won.\n",
    "* The show reports how much each item was sold for, allowing the viewer to keep a tally of their profit for each unit.\n",
    "* In addition to financial data, we also know where they are and thus can monitor location.\n",
    "* For more information, check out: https://www.imdb.com/title/tt1742340/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc0ae8-cda7-4afe-b896-00e03f8db859",
   "metadata": {},
   "source": [
    "<img src=\"https://m.media-amazon.com/images/M/MV5BNTc4OTE0MzcxOF5BMl5BanBnXkFtZTcwMjQ0NTM0Ng@@._V1_FMjpg_UX558_.jpg\" alt=\"Hosts Allen and Ton of Auction Hunters\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7109d9d-9004-47c4-a5df-a17d54c3824c",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* A\n",
    "* B\n",
    "* C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b83da6-70a5-41c4-a19a-f59c9a1898da",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ebca4-f66e-4dd5-b9e6-19d163d185cc",
   "metadata": {},
   "source": [
    "1. Scrape episode data from wikipedia\n",
    "2. Organise and clean the data\n",
    "3. Dive into the data to answer the objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760424b-82e7-4cae-970b-2145516afd97",
   "metadata": {},
   "source": [
    "# 1. Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d29ee49-63dc-4297-9a67-c58fdd2efacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2a0b3a-e307-47ff-891c-0ef3f7b1a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/List_of_Auction_Hunters_episodes\"\n",
    "DATA_DIR = \"data\"\n",
    "FILENAME = 'auction_hunters_webpage_content.html'\n",
    "WEBPAGE = f'{DATA_DIR}/{FILENAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc8af3-b6fd-4bf7-931f-45104bbed71c",
   "metadata": {},
   "source": [
    "# 2. Scrape the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf80759-750e-4610-935e-539fb0ca8c73",
   "metadata": {},
   "source": [
    "## 2.1. Download the webpage to avoid multiple requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2630ff0-f754-4a47-a027-83823ec1ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP request to the URL\n",
    "response = requests.get(URL)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the content with Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Save the parsed data to a file\n",
    "    with open(WEBPAGE, 'w', encoding='utf-8') as website_data:\n",
    "        website_data.write(str(soup))\n",
    "else:\n",
    "    print(f\">> Status Code: {response.status_code}. Please check settings and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde7f92-c0e7-4617-9705-d71b597ae0ac",
   "metadata": {},
   "source": [
    "## 2.2. Load the local copy of the webpage for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8160e090-4583-4291-a9f4-316f40896c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WEBPAGE, 'r', encoding='utf-8') as auction_hunters_wiki_website:\n",
    "    website_read = auction_hunters_wiki_website.read()\n",
    "    soup = BeautifulSoup(website_read, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4accc76-9eb5-43b5-8c76-549a9fdb9d6c",
   "metadata": {},
   "source": [
    "## 2.3. Inspecting the parsed data to find our table of episode information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951e185-e351-41bc-b16d-0e1f2fd36413",
   "metadata": {},
   "source": [
    "* The season-episode information is found in tables under the class *\"wikitable plainrowheaders wikiepisodetable\"*.\n",
    "* We can see that 5 seasons are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e615588-ad54-4cd1-8041-ebe0d364e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> There are 5 season(s) in the soup! If this is the number you're expecting, continue!\n"
     ]
    }
   ],
   "source": [
    "all_seasons_scrape = soup.find_all(class_=r\"wikitable plainrowheaders wikiepisodetable\")\n",
    "number_of_seasons = len(all_seasons_scrape)\n",
    "print(f\">> There are {number_of_seasons} season(s) in the soup! If this is the number you're expecting, continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c55d3-b3d1-492a-962c-8265b5944760",
   "metadata": {},
   "source": [
    "## 2.4. Saving the parsed data for each season individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f309c0-740f-4218-8d12-db47accd13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_data = all_seasons_scrape[0]\n",
    "season_2_data = all_seasons_scrape[1]\n",
    "season_3_data = all_seasons_scrape[2]\n",
    "season_4_data = all_seasons_scrape[3]\n",
    "season_5_data = all_seasons_scrape[4]\n",
    "\n",
    "all_seasons_data = [(season_1_data, 1), (season_2_data, 2), (season_3_data, 3), (season_4_data, 4), (season_5_data, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c902c-962d-43a0-9ace-0cc9ccd46a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within each season data, there are 2 classes in the html which contain the data I want to scrape. \n",
    "# class vevent (episode, date, location), and \n",
    "# class expand-child (episode description with cost, sold, profit text).\n",
    "\n",
    "# print(type(season_1_data))\n",
    "# There is 1 class vevent for every episide, so the number of vevent instances will sum the episode number.\n",
    "# Furthermore, each expand-child will contain episode information, so the number of vevent instances MUST match the number of expand-child instances.\n",
    "def check_season_X_count_matches(season_X_data):\n",
    "    print(f\"There are: >> {len(season_X_data.find_all(class_=r'vevent'))} << episode, date and location information cells in this season.\")\n",
    "    # We can see that the season 1 data object has Tag property and find_all has 8 returns. This tells me that season 1 has 8 episodes. True.\n",
    "    print(f\"There are: >> {len(season_X_data.find_all(class_=r'expand-child'))} << episode descriptions with cost, sold, profit text in this season)\")\n",
    "\n",
    "    if len(season_X_data.find_all(class_=r'vevent')) == len(season_X_data.find_all(class_=r'expand-child')):\n",
    "        print(\"AMAZING! The number of episode data and episode descriptions counts match! We can proceed to extract the information.\")\n",
    "    else:\n",
    "        print(\"NO! Something is wrong. There are unequal amounts of episodes data and episode descriptions. \")\n",
    "        \n",
    "check_season_X_count_matches(season_1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820e454-b9f1-4395-89cf-f67aaa4960c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_information(season_X_data):\n",
    "    \"\"\"This function will process the episode data and descriptions for a single season. It will seperate it into 3 parts. General data (episode, title, date), \n",
    "    episode descripion, and the monetary data. Function returns a list containing these informations. For our purposes, season_X_data is the season that the user\n",
    "    wants to get information from. Earlier, we created season_1_data, season_2_data and so on. This function is designed to process those variables.\"\"\"\n",
    "    \n",
    "    # This first section will seperate out the episode #, title, location from the vevent class.\n",
    "    # I am making a list of episode data, where each element in the list is its own list of individual episode information.\n",
    "    season_X_episode_data = []\n",
    "    for episode_data in season_X_data.find_all(class_=r'vevent'):\n",
    "        episode_data_list = []\n",
    "        for data_tag in episode_data.select(\"td\"):\n",
    "            episode_data_list.append(data_tag.text)\n",
    "        season_X_episode_data.append(episode_data_list)\n",
    "    \n",
    "    # This second section will seperate out the episode descriptions from the expant-child class.\n",
    "    # I am making a list of episode descriptions (including the monetary values), where each element in the list is its own list of individual episode information.\n",
    "    season_X_episode_descriptions = []\n",
    "    season_X_episode_monetary = []\n",
    "    for episode_description in season_X_data.find_all(class_=r'expand-child'):\n",
    "        episode_description_list = []\n",
    "        for description in episode_description.select(\"p\"):\n",
    "            episode_description_list.append(description.text)\n",
    "        season_X_episode_descriptions.append(episode_description_list)\n",
    "\n",
    "        # Episode monetary covers the paid, sold and profit values from each episode. \n",
    "        # Remember, these are going to be lists of lists.\n",
    "        episode_monetary = []\n",
    "        for money_values in episode_description.find_all('dd'):\n",
    "            episode_monetary.append(money_values.text)\n",
    "        season_X_episode_monetary.append(episode_monetary)\n",
    "\n",
    "    # Now, we're going to join the information for each episode together and save it in a list. \n",
    "    season_X_full_list_data_combined = []\n",
    "    for i in range(0, len(season_X_episode_data)):\n",
    "        season_X_full_list_data_combined.append([season_X_episode_data[i], season_X_episode_descriptions[i], season_X_episode_monetary[i]])\n",
    "    return season_X_full_list_data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8d9d5-97e8-4aec-99cc-612134d85986",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_X_list_info_combined = get_season_information(season_4_data)\n",
    "pprint.pprint(season_X_list_info_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c101c6f-e1e6-41df-aca3-7d0a666e7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_season_episode_number(season_num, episode_num):\n",
    "    \"\"\"I want to generate an episode ID, such as season 1 episode 1 = 101, season 1 episode 12 = 112. We will generate as a string data type.\"\"\"\n",
    "    s_num = str(season_num)\n",
    "    if episode_num < 10:\n",
    "        ep_num = \"0\" + str(episode_num)\n",
    "    else:\n",
    "        ep_num = str(episode_num)\n",
    "    \n",
    "    return s_num + ep_num  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d79348-69a1-403e-89f9-2d1133fb30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_episode_data_to_dict(season_X_episode_prepared_data, season_num):\n",
    "    \"\"\"Enter season_X_data for the data to process, and the season number. season_num is a single integer, e.g. 1 for season 1, or 12 for season 12.\n",
    "    If any certain value throws an exception, put in a default value from the dictionary. Then, it can be detected easily in post-processing.\"\"\"\n",
    "    \n",
    "# So, season 4 episode 1 had no description, meaning the list made for it was length 0. So, a 0 length list raises an index error.\n",
    "# I decided to create a default values table so that if it throws an error, it just assigns a default value.\n",
    "# I can then reference the defaults later in pandas and replace specific values, or identify them in a csv in excel.\n",
    "    \n",
    "    except_value_defaults = {\n",
    "            'season_num': \"0\",\n",
    "            'episode_num': \"99999\",\n",
    "            'unique_ep_ID': \"88888\",\n",
    "            'episode_name': \"NoNameEpisode\", \n",
    "            'location': \"NoLocationEpisode\", \n",
    "            'air_date': \"NoAirDateEpisode\", \n",
    "            'description': \"NoDescriptionEpisode\", \n",
    "            'paid_$': \"NoPaidEpisode\", \n",
    "            'sold_$': \"NoSoldEpisode\", \n",
    "            'profit_$': \"NoProfitEpisode\"\n",
    "        }\n",
    "    \n",
    "    season_X_episode_info_list_of_dicts = []\n",
    "    for episode in season_X_episode_prepared_data:    \n",
    "        try:\n",
    "            ep_num = int(episode[0][0])\n",
    "        except:\n",
    "            ep_num = except_value_defaults['episode_num']\n",
    "                  \n",
    "        try:\n",
    "            ep_name = episode[0][1].replace('\"', \"\")\n",
    "        except:\n",
    "            ep_name = except_value_defaults['episode_name']\n",
    "\n",
    "        try:\n",
    "            ep_loc = episode[0][2]\n",
    "        except:\n",
    "            ep_loc = except_value_defaults['location']\n",
    "        \n",
    "        try:\n",
    "            ep_date_initial = episode[0][3]\n",
    "            ep_date_extract = re.findall(r'\\d{4}-\\d{2}-\\d{2}', ep_date_initial)\n",
    "            ep_date = datetime.datetime.strptime(ep_date_extract[0], '%Y-%m-%d').date()\n",
    "        except:\n",
    "            ep_date = except_value_defaults['air_date']\n",
    "        \n",
    "        try:\n",
    "            ep_desc = episode[1][0].replace(\"\\n\", \"\").replace(\"  \", \" \")\n",
    "        except:\n",
    "            ep_desc = except_value_defaults['description']\n",
    "                      \n",
    "        try:\n",
    "            ep_paid = episode[2][0].replace(\",\", \"\").replace(\"$\", \"\")\n",
    "        except:    \n",
    "            ep_paid = except_value_defaults['paid_$']\n",
    "            \n",
    "        try:\n",
    "            ep_sold = episode[2][1].replace(\",\", \"\").replace(\"$\", \"\")\n",
    "        except:             \n",
    "            ep_sold = except_value_defaults['sold_$']\n",
    "            \n",
    "        try:\n",
    "            ep_profit = episode[2][2].replace(\",\", \"\").replace(\"$\", \"\")\n",
    "        except:             \n",
    "            ep_profit = except_value_defaults['profit_$']\n",
    "\n",
    "        try:       \n",
    "            unique_ep_ID = generate_unique_season_episode_number(season_num, ep_num)\n",
    "        except: \n",
    "            unique_ep_ID = except_value_defaults['unique_ep_ID']\n",
    "            \n",
    "            \n",
    "        ep_dict = {\n",
    "            'season_num': season_num,\n",
    "            'episode_num': ep_num,\n",
    "            'unique_ep_ID': unique_ep_ID,\n",
    "            'episode_name': ep_name, \n",
    "            'location': ep_loc, \n",
    "            'air_date': ep_date, \n",
    "            'description': ep_desc, \n",
    "            'paid_$': ep_paid, \n",
    "            'sold_$': ep_sold, \n",
    "            'profit_$': ep_profit\n",
    "        }\n",
    "\n",
    "        season_X_episode_info_list_of_dicts.append(ep_dict)\n",
    "\n",
    "    return season_X_episode_info_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb198dc-bcf1-40fe-9c4c-9021176413fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will assign the season list of dictionaries variable and generate them using the season data and season number.\n",
    "season_X_episodes_as_list_of_dictionaries = convert_episode_data_to_dict(season_X_list_info_combined, 1)\n",
    "pprint.pprint(season_X_episodes_as_list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025c0ad-8882-40b0-82e2-6620654a52e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, we will export the results to a pandas dataframe.\n",
    "season_X_as_df = pd.DataFrame.from_records(season_X_episodes_as_list_of_dictionaries)\n",
    "print(season_X_as_df)\n",
    "season_X_as_df.to_csv('season_X_auction_hunters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cf8b1-360a-4b8f-8a52-d7bf761030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in all_seasons_data:\n",
    "    check_season_X_count_matches(season[0])\n",
    "    season_X_list_info_combined = get_season_information(season[0])\n",
    "    convert_episode_data_to_dict(season_X_list_info_combined, season[1])\n",
    "    season_X_episodes_as_list_of_dictionaries = convert_episode_data_to_dict(season_X_list_info_combined, season[1])\n",
    "    season_X_as_df = pd.DataFrame.from_records(season_X_episodes_as_list_of_dictionaries)\n",
    "    \n",
    "    \n",
    "    # If you set exist_ok=True, you can specify an existing directory without encountering an error.\n",
    "    os.makedirs('data', exist_ok=True)  \n",
    "    title_concat = \"season_\" + str(season[1]) + \"_auction_hunters\"\n",
    "    season_X_as_df.to_csv('data/'+f\"{title_concat}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494c275-7c08-4dbe-acf0-737536e17059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
